# Introduction

The member should be able to: calculate and evaluate the predicted trend value for a time series, modeled as either a linear trend or a log-linear trend, given the estimated trend coefficients; describe factors that determine whether a linear or a log-linear trend should be used with a particular time series and evaluate limitations of trend models; explain the requirement for a time series to be covariance stationary and describe the significance of a series that is not stationary; describe the structure of an autoregressive (AR) model of order p and calculate one- and two-period-ahead forecasts given the estimated coefficients; explain how autocorrelations of the residuals can be used to test whether the autoregressive model fits the time series; explain mean reversion and calculate a mean-reverting level; contrast in-sample and out-of-sample forecasts and compare the forecasting accuracy of different time-series models based on the root mean squared error criterion; explain the instability of coefficients of time-series models; describe characteristics of random walk processes and contrast them to covariance stationary processes; describe implications of unit roots for time-series analysis, explain when unit roots are likely to occur and how to test for them, and demonstrate how a time series with a unit root can be transformed so it can be analyzed with an AR model; describe the steps of the unit root test for nonstationarity and explain the relation of the test to autoregressive time-series models; explain how to test and correct for seasonality in a time-series model and calculate and interpret a forecasted value using an AR model with a seasonal lag; explain autoregressive conditional heteroskedasticity (ARCH) and describe how ARCH models can be applied to predict the variance of a time series; explain how time-series variables should be analyzed for nonstationarity and/or cointegration before use in a linear regression; and determine an appropriate time-series model to analyze a given investment problem and justify that choice.

## Summary

The predicted trend value of a time series in period t is b ‸ 0 + b ‸ 1 t in a linear trend model; the predicted trend value of a time series in a log-linear trend model is e b ‸ 0 + b ‸ 1 t ., Time series that tend to grow by a constant amount from period to period should be modeled by linear trend models, whereas time series that tend to grow at a constant rate should be modeled by log-linear trend models., Trend models often do not completely capture the behavior of a time series, as indicated by serial correlation of the error term. If the Durbin–Watson statistic from a trend model differs significantly from 2, indicating serial correlation, we need to build a different kind of model., An autoregressive model of order p, denoted AR(p), uses p lags of a time series to predict its current value: xt = b 0 + b 1 xt −1 + b 2 xt −2 + . . . + bpxt – p + ε t ., A time series is covariance stationary if the following three conditions are satisfied: First, the expected value of the time series must be constant and finite in all periods. Second, the variance of the time series must be constant and finite in all periods. Third, the covariance of the time series with itself for a fixed number of periods in the past or future must be constant and finite in all periods. Inspection of a nonstationary time-series plot may reveal an upward or downward trend (nonconstant mean) and/or nonconstant variance. The use of linear regression to estimate an autoregressive time-series model is not valid unless the time series is covariance stationary., For a specific autoregressive model to be a good fit to the data, the autocorrelations of the error term should be 0 at all lags., A time series is mean reverting if it tends to fall when its level is above its long-run mean and rise when its level is below its long-run mean. If a time series is covariance stationary, then it will be mean reverting., The one-period-ahead forecast of a variable xt from an AR(1) model made in period t for period t + 1 is x ‸ t + 1 = b ‸ 0 + b ‸ 1 x t . This forecast can be used to create the two-period-ahead forecast from the model made in period t, x ‸ t + 2 = b ‸ 0 + b ‸ 1 x t + 1 . Similar results hold for AR(p) models., In-sample forecasts are the in-sample predicted values from the estimated time-series model. Out-of-sample forecasts are the forecasts made from the estimated time-series model for a time period different from the one for which the model was estimated. Out-of-sample forecasts are usually more valuable in evaluating the forecasting performance of a time-series model than are in-sample forecasts. The root mean squared error (RMSE), defined as the square root of the average squared forecast error, is a criterion for comparing the forecast accuracy of different time-series models; a smaller RMSE implies greater forecast accuracy., Just as in regression models, the coefficients in time-series models are often unstable across different sample periods. In selecting a sample period for estimating a time-series model, we should seek to assure ourselves that the time series was stationary in the sample period., A random walk is a time series in which the value of the series in one period is the value of the series in the previous period plus an unpredictable random error. If the time series is a random walk, it is not covariance stationary. A random walk with drift is a random walk with a nonzero intercept term. All random walks have unit roots. If a time series has a unit root, then it will not be covariance stationary., If a time series has a unit root, we can sometimes transform the time series into one that is covariance stationary by first-differencing the time series; we may then be able to estimate an autoregressive model for the first-differenced series., An n-period moving average of the current and past (n − 1) values of a time series, xt , is calculated as [xt + xt −1 + . . . + xt −( n −1)]/n., A moving-average model of order q, denoted MA(q), uses q lags of a random error term to predict its current value., The order q of a moving-average model can be determined using the fact that if a time series is a moving-average time series of order q, its first q autocorrelations are nonzero while autocorrelations beyond the first q are zero., The autocorrelations of most autoregressive time series start large and decline gradually, whereas the autocorrelations of an MA(q) time series suddenly drop to 0 after the first q autocorrelations. This helps in distinguishing between autoregressive and moving-average time series., If the error term of a time-series model shows significant serial correlation at seasonal lags, the time series has significant seasonality. This seasonality can often be modeled by including a seasonal lag in the model, such as adding a term lagged four quarters to an AR(1) model on quarterly observations., The forecast made in time t for time t + 1 using a quarterly AR(1) model with a seasonal lag would be x t + 1 = b ‸ 0 + b ‸ 1 x t + b ‸ 2 x t − 3 ., ARMA models have several limitations: The parameters in ARMA models can be very unstable; determining the AR and MA order of the model can be difficult; and even with their additional complexity, ARMA models may not forecast well., The variance of the error in a time-series model sometimes depends on the variance of previous errors, representing autoregressive conditional heteroskedasticity (ARCH). Analysts can test for first-order ARCH in a time-series model by regressing the squared residual on the squared residual from the previous period. If the coefficient on the squared residual is statistically significant, the time-series model has ARCH(1) errors., If a time-series model has ARCH(1) errors, then the variance of the errors in period t + 1 can be predicted in period t using the formula σ ‸ t + 1 2 = a ‸ 0 + a ‸ 1 ε ‸ t 2 ., If linear regression is used to model the relationship between two time series, a test should be performed to determine whether either time series has a unit root: If neither of the time series has a unit root, then we can safely use linear regression. If one of the two time series has a unit root, then we should not use linear regression. If both time series have a unit root and the time series are cointegrated, we may safely use linear regression; however, if they are not cointegrated, we should not use linear regression. The (Engle–Granger) Dickey–Fuller test can be used to determine whether time series are cointegrated., If neither of the time series has a unit root, then we can safely use linear regression., If one of the two time series has a unit root, then we should not use linear regression., If both time series have a unit root and the time series are cointegrated, we may safely use linear regression; however, if they are not cointegrated, we should not use linear regression. The (Engle–Granger) Dickey–Fuller test can be used to determine whether time series are cointegrated.

## Learning Outcomes

The member should be able to: calculate and evaluate the predicted trend value for a time series, modeled as either a linear trend or a log-linear trend, given the estimated trend coefficients; describe factors that determine whether a linear or a log-linear trend should be used with a particular time series and evaluate limitations of trend models; explain the requirement for a time series to be covariance stationary and describe the significance of a series that is not stationary; describe the structure of an autoregressive (AR) model of order p and calculate one- and two-period-ahead forecasts given the estimated coefficients; explain how autocorrelations of the residuals can be used to test whether the autoregressive model fits the time series; explain mean reversion and calculate a mean-reverting level; contrast in-sample and out-of-sample forecasts and compare the forecasting accuracy of different time-series models based on the root mean squared error criterion; explain the instability of coefficients of time-series models; describe characteristics of random walk processes and contrast them to covariance stationary processes; describe implications of unit roots for time-series analysis, explain when unit roots are likely to occur and how to test for them, and demonstrate how a time series with a unit root can be transformed so it can be analyzed with an AR model; describe the steps of the unit root test for nonstationarity and explain the relation of the test to autoregressive time-series models; explain how to test and correct for seasonality in a time-series model and calculate and interpret a forecasted value using an AR model with a seasonal lag; explain autoregressive conditional heteroskedasticity (ARCH) and describe how ARCH models can be applied to predict the variance of a time series; explain how time-series variables should be analyzed for nonstationarity and/or cointegration before use in a linear regression; and determine an appropriate time-series model to analyze a given investment problem and justify that choice.

## Technical Note

**Technical Note**

**LOS:** Calculate and evaluate trend values, understand time series requirements, build autoregressive models, compare forecasting accuracy, analyze cointegration, and select appropriate time-series models.

**Trend models:**
* Linear trend: Predicted trend value = b0 + b1t
* Log-linear trend: Predicted trend value = e(b0 + b1t)
* Linear trend for constant growth, log-linear trend for constant growth rate

**Covariance stationarity:**
* Required for linear regression analysis
* Conditions: constant mean, constant variance, constant covariance
* Nonstationary series may exhibit trends or nonconstant variance

**Autoregressive models (AR):**
* Structure: xt = b0 + b1xt-1 + ... + bpxp-1 + εt
* Autocorrelations of residuals used to test model fit

**Mean reversion:**
* Tendency for a series to return to its long-run mean

**Forecasting:**
* One-period-ahead forecast: xt+1 = b0 + b1xt
* Root mean squared error (RMSE) for comparing forecasting accuracy

**Random walks and unit roots:**
* Random walks: Nonstationary series with no covariance stationarity
* Unit roots imply nonstationarity
* Transformation (e.g., first-differencing) may be necessary for analysis with AR models

**Seasonality:**
* Significant serial correlation at seasonal lags
* Modeled by including seasonal lags in time-series models

**Heteroskedasticity (ARCH):**
* Variance of error depends on previous error variances
* Can be modeled using ARCH models

**Cointegration:**
* Time series with unit roots may be cointegrated
* Linear regression can be used if neither series has a unit root or both series are cointegrated

**Model selection:**
* Consider nonstationarity, cointegration, and sample period stability
* Evaluate forecasting accuracy and justify choice based on investment problem