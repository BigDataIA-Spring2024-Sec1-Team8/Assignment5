# Introduction

The member should be able to: describe a simple linear regression model and the roles of the dependent and independent variables in the model; describe the least squares criterion, how it is used to estimate regression coefficients, and their interpretation; explain the assumptions underlying the simple linear regression model, and describe how residuals and residual plots indicate if these assumptions may have been violated; calculate and interpret the coefficient of determination and the F-statistic in a simple linear regression; describe the use of analysis of variance (ANOVA) in regression analysis, interpret ANOVA results, and calculate and interpret the standard error of estimate in a simple linear regression; formulate a null and an alternative hypothesis about a population value of a regression coefficient, and determine whether the null hypothesis is rejected at a given level of significance; calculate and interpret the predicted value for the dependent variable, and a prediction interval for it, given an estimated linear regression model and a value for the independent variable; describe different functional forms of simple linear regressions.

## Summary

The dependent variable in a linear regression is the variable whose variability the regression model tries to explain. The independent variable is the variable whose variation the researcher uses to explain the variation of the dependent variable., If there is one independent variable in a linear regression and there are n observations of the dependent and independent variables, the regression model is Yi = b0 + b1Xi + ei, i = 1, . . . , n, where Yi is the dependent variable, Xi is the independent variable, and ei is the error term. In this model, the coefficients and are the population intercept and slope, respectively., The intercept is the expected value of the dependent variable when the independent variable has a value of zero. The slope coefficient is the estimate of the population slope of the regression line and is the expected change in the dependent variable for a one-unit change in the independent variable., The assumptions of the classic simple linear regression model are as follows:

## Learning Outcomes

The member should be able to: describe a simple linear regression model and the roles of the dependent and independent variables in the model; describe the least squares criterion, how it is used to estimate regression coefficients, and their interpretation; explain the assumptions underlying the simple linear regression model, and describe how residuals and residual plots indicate if these assumptions may have been violated; calculate and interpret the coefficient of determination and the F-statistic in a simple linear regression; describe the use of analysis of variance (ANOVA) in regression analysis, interpret ANOVA results, and calculate and interpret the standard error of estimate in a simple linear regression; formulate a null and an alternative hypothesis about a population value of a regression coefficient, and determine whether the null hypothesis is rejected at a given level of significance; calculate and interpret the predicted value for the dependent variable, and a prediction interval for it, given an estimated linear regression model and a value for the independent variable; describe different functional forms of simple linear regressions.

## Technical Note

1. **Linear Regression Model**: A linear regression model expresses a linear relationship between a dependent variable and one or more independent variables.
2. **Dependent and Independent Variables**: The dependent variable is the variable being predicted, while the independent variable(s) are the variables used to predict it.
3. **Least Squares Criterion**: The least squares criterion minimizes the sum of the squared residuals (errors) to estimate the regression coefficients.
4. **Intercept and Slope**: The intercept is the value of the dependent variable when the independent variable is zero, and the slope represents the change in the dependent variable for a unit change in the independent variable.
5. **Assumptions of Linear Regression**: The assumptions include linearity, homoscedasticity, independence, and normality of residuals.
6. **Residuals and Residual Plots**: Residuals measure the difference between observed and predicted values, and residual plots help identify potential violations of model assumptions.
7. **Coefficient of Determination (R-squared)**: R-squared represents the proportion of variation in the dependent variable explained by the regression model.
8. **F-statistic**: The F-statistic tests whether the regression model significantly explains the variation in the dependent variable.
9. **Hypothesis Testing**: Hypothesis testing involves formulating hypotheses about regression coefficients and determining whether they are rejected at a given level of significance.
10. **Predicted Value and Prediction Interval**: Given a fitted model and an independent variable value, the predicted value is the estimate of the dependent variable, while the prediction interval provides a range within which the true value is likely to fall.